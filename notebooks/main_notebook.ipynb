{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "review_sentiment_glove.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "He5BRhnJf1V8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "_1FjSoIQD0Lb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook is divided into 3 parts. \n",
        "\n",
        "\n",
        "\n",
        "1.   In the first part I have trained the model on data_0 file(5M reviews) which we generated in data_separation.ipynb.\n",
        "2.   In the second part I have trained the model on remaining files (data_1 ---- data_7, each of 5M reviews)\n",
        "3.   In the 3rd part I will load the test data and check our model performance on the test data."
      ]
    },
    {
      "metadata": {
        "id": "FC7GZTNkhaI2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup and imports"
      ]
    },
    {
      "metadata": {
        "id": "ZNDHsDyVr4FI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9NEe8pWiw4FA",
        "colab_type": "code",
        "outputId": "97c93ac7-62ff-43e6-a7f8-ed0a52319fb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# get all imports\n",
        "import re\n",
        "from tqdm import tqdm     # to show progress of a loop\n",
        "import bz2                # to extract data from file\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "import gensim\n",
        "import sys\n",
        "import pickle\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import string\n",
        "# Load stop words\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "HPypmUhqgZ2R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the GloVE Embedding and the training data"
      ]
    },
    {
      "metadata": {
        "id": "jIQdJHUpws3Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "  \n",
        "# load glove pretrained model\n",
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/content/drive/My Drive/Machine Learning Projects/Word Embedding Models/glove/glove.6B.200d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xq30vOzvsJfS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make directory to store data\n",
        "!mkdir data\n",
        "# get the splitted data\n",
        "!cp -r '/content/drive/My Drive/Machine Learning Projects/Amazon Sentiment Analysis/data/seperated_data' /content/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "He5BRhnJf1V8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ALL HELPER METHODS\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IYvkeK_gsJcR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define all helper methods\n",
        "def getReviewX(review):\n",
        "  # seperate label\n",
        "  review = review.split(' ', 1)[1]\n",
        "  # replace all numbers\n",
        "  review = re.sub('[0-9]+', '0', review)\n",
        "  # replace all urls\n",
        "  if 'http' in review or 'www.' in review:\n",
        "    regex_url = '((http(s)+(\\:\\/\\/))?(www\\.)?([\\w\\-\\.\\/])*(\\.[a-zA-Z]{2,3}\\/?))[^\\s\\b\\n|]*[^.,;:\\?\\!\\@\\^\\$ -]'\n",
        "    review = re.sub(regex_url, '<url>', review)\n",
        "  return review\n",
        "\n",
        "def getReviewY(review):\n",
        "  #seperate the labels\n",
        "  review = review.split(' ', 1)[0]\n",
        "  return [1, 0] if review.split(' ', 1)[0] == '__label__1' else [0, 1]  \n",
        "\n",
        "def convertToNumpyY(y):\n",
        "  return [1, 0] if y == 0 else [0, 1]\n",
        "\n",
        "def splitLabelsReviews(lines):\n",
        "  reviews=[]\n",
        "  labels=[]\n",
        "  for line in tqdm(lines):\n",
        "    review = getReviewX(line)\n",
        "    label = getReviewY(line)\n",
        "    reviews.append(review[:1024])   # restrict the size of sent to 1024 chars\n",
        "    labels.append(label)\n",
        "  return reviews, labels\n",
        "\n",
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    \"\"\"\n",
        "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
        "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
        "    \n",
        "    Arguments:\n",
        "    X -- array of sentences (strings), of shape (m, 1)\n",
        "    word_to_index -- a dictionary containing the each word mapped to its index\n",
        "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
        "    \n",
        "    Returns:\n",
        "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Make translation table to replace puctuation\n",
        "    replace_char = {key: None for key in string.punctuation}\n",
        "    replace_char['\"'] = None\n",
        "    table = str.maketrans(replace_char)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    \n",
        "    \n",
        "    m = len(X)                                   # number of training examples\n",
        "    \n",
        "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
        "    X_indices = np.zeros(shape=(m, max_len))\n",
        "    \n",
        "    for i in range(m):                               # loop over training examples\n",
        "        \n",
        "        # Remove punctuation\n",
        "        X[i] = X[i].translate(table)\n",
        "        \n",
        "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
        "        sentence_words = X[i].lower().split()        \n",
        "        \n",
        "        # Initialize j to 0\n",
        "        j = 0\n",
        "        \n",
        "        # Store indices of unknown words in list and then replace it by\n",
        "        # the average of all words\n",
        "        unknown_words_index = []\n",
        "        \n",
        "        # Loop over the words of sentence_words\n",
        "        for w in sentence_words:\n",
        "            # Skip Stopwords\n",
        "            if w in stop_words:\n",
        "              continue\n",
        "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "            if w in word_to_index and j < max_len:\n",
        "              X_indices[i, j] = word_to_index[w]\n",
        "            else:\n",
        "              # Handle unknown key (keep it as zeros)\n",
        "              pass\n",
        "            # Increment j to j + 1\n",
        "            j += 1            \n",
        "    \n",
        "    return X_indices\n",
        "  \n",
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
        "    \n",
        "    Arguments:\n",
        "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    embedding_layer -- pretrained layer Keras instance\n",
        "    \"\"\"\n",
        "    \n",
        "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
        "    \n",
        "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
        "    emb_matrix = np.zeros(shape=(vocab_len, emb_dim))\n",
        "    \n",
        "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "    for word, index in word_to_index.items():\n",
        "        emb_matrix[index, :] = word_to_vec_map[word]\n",
        "\n",
        "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
        "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
        "    \n",
        "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
        "    embedding_layer.build((None,))\n",
        "    \n",
        "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
        "    embedding_layer.set_weights([emb_matrix])\n",
        "    \n",
        "    return embedding_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1_UWOW_9gKyO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## STEP 1"
      ]
    },
    {
      "metadata": {
        "id": "D9wPwD84sJS3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_file = open('/content/data/seperated_data/data_0', 'r')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uqjuriZUsJZJ",
        "colab_type": "code",
        "outputId": "89bcf141-f249-4ad7-a802-e2381474bce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# test getReviewX\n",
        "dummy = 'this is this www.google.com https://www.google.com 456789' \n",
        "print(getReviewX(dummy))\n",
        "#test getReviewY\n",
        "dummy = '__label__1 asasddasd asd'\n",
        "print(getReviewY(dummy))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is this <url> <url> 0\n",
            "[1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oDkO7W7zsJPo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ----IGNORE----- \n",
        "\n",
        "# train_lines = train_file.readlines()\n",
        "# test_lines = test_file.readlines()\n",
        "# print(type(train_lines))\n",
        "# print(type(train_lines[0]))\n",
        "# convert list of bytes to list of string \n",
        "# train_lines = [x.decode('utf-8') for x in train_file.readlines()]\n",
        "# test_lines = [x.decode('utf-8') for x in test_file.readlines()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "US0UoYBPxzPd",
        "colab_type": "code",
        "outputId": "f8a594ff-eb94-4781-c8bb-23fac2eb3a30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# seperate data and labels \n",
        "# no need to decode train file as we have already done it while seperation\n",
        "reviews_train, y_train = splitLabelsReviews([x for x in train_file.readlines() if x != '\\n'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500000/500000 [00:09<00:00, 55274.57it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yvn7BwfnxzLW",
        "colab_type": "code",
        "outputId": "19ff3e39-e861-4583-e090-ca57151eb569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# shuffle data\n",
        "reviews_train, y_train = shuffle(reviews_train, y_train)\n",
        "# convert y to numpy\n",
        "y_train = np.array(y_train)\n",
        "# data stats\n",
        "print('Number of train reviews ' + str(len(reviews_train)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train reviews 500000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5yp6nZaoxzDq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a633f39b-eeb0-462f-a9ee-fe74e2718552"
      },
      "cell_type": "code",
      "source": [
        "max_len = len(max(reviews_train, key=len).split())\n",
        "print(max_len)\n",
        "\n",
        "# FIX THE MAX LENGTH OF SENTENCE\n",
        "max_len = 80"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AyCrlCMnxy0R",
        "colab_type": "code",
        "outputId": "525d9213-e320-44db-e595-52cc330d52a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "\n",
        "input_sent = Input(shape=(max_len,))\n",
        "\n",
        "# Create the embedding layer pretrained with glove Vectors (≈1 line)\n",
        "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "    \n",
        "# Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
        "embeddings = embedding_layer(input_sent)\n",
        "\n",
        "X = Bidirectional(GRU(128))(embeddings)\n",
        "X = Dropout(0.2)(X)\n",
        "X = Dense(2)(X)\n",
        "X = Activation('softmax')(X)\n",
        "\n",
        "model = Model(inputs = input_sent, outputs = X)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 80, 200)           80000200  \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 256)               252672    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 514       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 80,253,386\n",
            "Trainable params: 253,186\n",
            "Non-trainable params: 80,000,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U4q9Y9wVxzAA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert word to indices\n",
        "reviews_train = sentences_to_indices(reviews_train, word_to_index, max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "voHjiLNbxyvF",
        "colab_type": "code",
        "outputId": "c9a25eb6-b15b-450c-b273-6f75d953869d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(reviews_train, y_train, epochs = 2, batch_size = 64)\n",
        "model.save('/content/drive/My Drive/Machine Learning Projects/Amazon Sentiment Analysis/checkpoint_2_epoch.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "500000/500000 [==============================] - 2179s 4ms/step - loss: 0.2777 - acc: 0.8828\n",
            "Epoch 2/2\n",
            "500000/500000 [==============================] - 2147s 4ms/step - loss: 0.2260 - acc: 0.9082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p6ROpUTkhO0e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## STEP 2"
      ]
    },
    {
      "metadata": {
        "id": "po86iZoxYlAk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load previous trained model\n",
        "model = load_model('/content/drive/My Drive/Machine Learning Projects/Amazon Sentiment Analysis/checkpoint_2_epoch.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2kZ7ALaUZWlq",
        "colab_type": "code",
        "outputId": "2794132c-65d3-4613-ca3a-e236936bdc38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "cell_type": "code",
      "source": [
        "# code to train model on multiple small data files\n",
        "for i in range(1, 8):\n",
        "  print('Starting training on data '+str(i))\n",
        "  train_file = open('/content/data/seperated_data/data_'+str(i), 'r')\n",
        "  reviews_train, y_train = splitLabelsReviews([x for x in train_file.readlines() if x != '\\n'])\n",
        "  train_file.close()\n",
        "  # shuffle data\n",
        "  reviews_train, y_train = shuffle(reviews_train, y_train)\n",
        "  # convert y to numpy\n",
        "  y_train = np.array(y_train)\n",
        "  # data stats\n",
        "  print('Number of train reviews ' + str(len(reviews_train)))\n",
        "  max_len = 80\n",
        "  # convert word to indices\n",
        "  reviews_train = sentences_to_indices(reviews_train, word_to_index, max_len)\n",
        "  \n",
        "  model.fit(reviews_train, y_train, epochs = 2, batch_size = 64)\n",
        "  model.save('checkpoint_2_epoch_data_'+str(i)+'.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training on data 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500000/500000 [00:09<00:00, 55275.84it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of train reviews 500000\n",
            "Epoch 1/2\n",
            "500000/500000 [==============================] - 2178s 4ms/step - loss: 0.2125 - acc: 0.9141\n",
            "Epoch 2/2\n",
            "500000/500000 [==============================] - 2189s 4ms/step - loss: 0.1927 - acc: 0.9233\n",
            "Starting training on data 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500000/500000 [00:08<00:00, 58338.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of train reviews 500000\n",
            "Epoch 1/2\n",
            "500000/500000 [==============================] - 2178s 4ms/step - loss: 0.2115 - acc: 0.9151\n",
            "Epoch 2/2\n",
            "500000/500000 [==============================] - 2176s 4ms/step - loss: 0.1924 - acc: 0.9239\n",
            "Starting training on data 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500000/500000 [00:08<00:00, 55993.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of train reviews 500000\n",
            "Epoch 1/2\n",
            "500000/500000 [==============================] - 2239s 4ms/step - loss: 0.2077 - acc: 0.9169\n",
            "Epoch 2/2\n",
            "500000/500000 [==============================] - 2166s 4ms/step - loss: 0.1888 - acc: 0.9251\n",
            "Starting training on data 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500000/500000 [00:08<00:00, 60098.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of train reviews 500000\n",
            "Epoch 1/2\n",
            "500000/500000 [==============================] - 2082s 4ms/step - loss: 0.2076 - acc: 0.9172\n",
            "Epoch 2/2\n",
            "193920/500000 [==========>...................] - ETA: 20:19 - loss: 0.1855 - acc: 0.9274Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kCKHmZ3hZWhX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# copy all the checkpoints to drive\n",
        "!cp /content/checkpoint_2_epoch_data_*.h5 \"/content/drive/My Drive/Machine Learning Projects/Amazon Sentiment Analysis\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TjMgRBtXdwuN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **STEP 3**"
      ]
    },
    {
      "metadata": {
        "id": "achWxY6XdYH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "aa843427-9614-44e2-db2b-b64e93eced0f"
      },
      "cell_type": "code",
      "source": [
        "# LOAD THE TESTING DATA\n",
        "!cp '/content/drive/My Drive/Machine Learning Projects/Amazon Sentiment Analysis/data/test.ft.txt.bz2.zip' /content/data\n",
        "# extract data\n",
        "!unzip /content/data/test.ft.txt.bz2.zip\n",
        "# move extracted file\n",
        "!mv /content/test.ft.txt.bz2 /content/data/test.ft.txt.bz2\n",
        "test_file = bz2.BZ2File('/content/data/test.ft.txt.bz2')\n",
        "\n",
        "reviews_test, y_test = splitLabelsReviews([x.decode('utf-8') for x in test_file.readlines()])\n",
        "reviews_test, y_test = shuffle(reviews_test, y_test)\n",
        "y_test = np.array(y_test)\n",
        "print('Number of test reviews ' + str(len(reviews_test)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/data/test.ft.txt.bz2.zip\n",
            "  inflating: test.ft.txt.bz2         \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 400000/400000 [00:07<00:00, 57113.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of test reviews 400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1C0Z98VsdYFR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert word to indices\n",
        "reviews_test = sentences_to_indices(reviews_test, word_to_index, max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nZ7jRBDikIIJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the trained model\n",
        "model = load_model('/content/drive/My Drive/Machine Learning Projects/Amazon Sentiment Analysis/checkpoint_2_epoch_data_7.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jnisUr4adYBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32858a3d-8392-4b9a-e325-a6791f4b65be"
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict(reviews_test, verbose=1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000/400000 [==============================] - 1492s 4ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HZmxuAINrE9J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions_list = np.array([convertToNumpyY(x) for x in np.argmax(predictions, axis=1)])\n",
        "predict_diff = [np.array_equal(x, y) for (x, y) in zip(y_test, predictions_list)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zvyUcacSxENX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f4236e7-b06e-435e-f13e-45b80f8d5622"
      },
      "cell_type": "code",
      "source": [
        "print(\"ACCURACY on Test Data ---- {}\".format(np.mean(predict_diff)))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACCURACY on Test Data ---- 0.91658\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}