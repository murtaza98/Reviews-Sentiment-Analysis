{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "review_sentiment_glove.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "_1FjSoIQD0Lb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook is divided into 3 parts. \n",
        "\n",
        "\n",
        "\n",
        "1.   In the first part we will train the define and train the   model on data_0 file which we generated in data_separation.ipynb.\n",
        "2.   In the second part we will train the model on remaining files (data_1 ---- data_7)\n",
        "3.   In the 3rd part we will load the test data and check our model performance on the test data."
      ]
    },
    {
      "metadata": {
        "id": "ZNDHsDyVr4FI",
        "colab_type": "code",
        "outputId": "6cf92715-16f9-4319-9c36-8c741533cf73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9NEe8pWiw4FA",
        "colab_type": "code",
        "outputId": "75101164-bdeb-4ca4-f6db-f48d7012ebdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# get all imports\n",
        "import re\n",
        "from tqdm import tqdm     # to show progress of a loop\n",
        "import bz2                # to extract data from file\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "import gensim\n",
        "import sys\n",
        "import pickle\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "jPJqTrkizDu6",
        "colab_type": "code",
        "outputId": "24594b3f-51ec-4b2f-d095-a0d08c1e956c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Load stop words\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "jIQdJHUpws3Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "42ZVIKDPsJnL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load glove pretrained model\n",
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/content/drive/My Drive/Machine Learning Projects/Word Embedding Models/glove/glove.6B.200d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xq30vOzvsJfS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make directory to store data\n",
        "!mkdir data\n",
        "# get the data\n",
        "!cp -r '/content/drive/My Drive/Machine Learning Projects/Amazon Sentiment Analysis/data/seperated_data' /content/data\n",
        "# !cp '/content/drive/My Drive/Machine Learning Projects/Amazon Sentiment Analysis/data/test.ft.txt.bz2.zip' /content/data\n",
        "# extract data\n",
        "# !unzip /content/data/train.ft.txt.bz2.zip\n",
        "# !unzip /content/data/test.ft.txt.bz2.zip\n",
        "# move extracted file\n",
        "# !mv /content/train.ft.txt.bz2 /content/data/train.ft.txt.bz2\n",
        "# !mv /content/test.ft.txt.bz2 /content/data/test.ft.txt.bz2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IYvkeK_gsJcR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getReviewX(review):\n",
        "  # seperate label\n",
        "  review = review.split(' ', 1)[1]\n",
        "  # replace all numbers\n",
        "  review = re.sub('[0-9]+', '0', review)\n",
        "  # replace all urls\n",
        "  if 'http' in review or 'www.' in review:\n",
        "    regex_url = '((http(s)+(\\:\\/\\/))?(www\\.)?([\\w\\-\\.\\/])*(\\.[a-zA-Z]{2,3}\\/?))[^\\s\\b\\n|]*[^.,;:\\?\\!\\@\\^\\$ -]'\n",
        "    review = re.sub(regex_url, '<url>', review)\n",
        "  return review\n",
        "\n",
        "def getReviewY(review):\n",
        "  #seperate the labels\n",
        "  review = review.split(' ', 1)[0]\n",
        "  return [1, 0] if review.split(' ', 1)[0] == '__label__1' else [0, 1]  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uqjuriZUsJZJ",
        "colab_type": "code",
        "outputId": "433572f0-b166-46f2-ecab-ea0ff558b676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# test getReviewX\n",
        "dummy = 'this is this www.google.com https://www.google.com 456789' \n",
        "print(getReviewX(dummy))\n",
        "#test getReviewY\n",
        "dummy = '__label__1 asasddasd asd'\n",
        "print(getReviewY(dummy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is this <url> <url> 0\n",
            "[1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zAlLG5CrsJV3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def splitLabelsReviews(lines):\n",
        "  reviews=[]\n",
        "  labels=[]\n",
        "  for line in tqdm(lines):\n",
        "    review = getReviewX(line)\n",
        "    label = getReviewY(line)\n",
        "    reviews.append(review[:1024])   # restrict the size of sent to 1024 chars\n",
        "    labels.append(label)\n",
        "  return reviews, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D9wPwD84sJS3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_file = open('/content/data/data_0', 'r')\n",
        "test_file = bz2.BZ2File('/content/data/test.ft.txt.bz2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oDkO7W7zsJPo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train_lines = train_file.readlines()\n",
        "# test_lines = test_file.readlines()\n",
        "# print(type(train_lines))\n",
        "# print(type(train_lines[0]))\n",
        "# convert list of bytes to list of string \n",
        "# train_lines = [x.decode('utf-8') for x in train_file.readlines()]\n",
        "# test_lines = [x.decode('utf-8') for x in test_file.readlines()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "US0UoYBPxzPd",
        "colab_type": "code",
        "outputId": "50d3199f-049c-4129-fa78-ddb3673360be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# seperate data and labels \n",
        "# no need to decode train file as we have already done it while seperation\n",
        "reviews_train, y_train = splitLabelsReviews([x for x in train_file.readlines() if x != '\\n'])\n",
        "reviews_test, y_test = splitLabelsReviews([x.decode('utf-8') for x in test_file.readlines()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500000/500000 [00:08<00:00, 56210.88it/s]\n",
            "100%|██████████| 400000/400000 [00:06<00:00, 58160.66it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yvn7BwfnxzLW",
        "colab_type": "code",
        "outputId": "c2d9e3bb-c3e8-487b-a42d-c7d42b7c27f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# shuffle data\n",
        "reviews_train, y_train = shuffle(reviews_train, y_train)\n",
        "reviews_test, y_test = shuffle(reviews_test, y_test)\n",
        "# convert y to numpy\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "# data stats\n",
        "# data stats\n",
        "print('Number of train reviews ' + str(len(reviews_train)))\n",
        "print('Number of test reviews ' + str(len(reviews_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train reviews 500000\n",
            "Number of test reviews 400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FIqRPw40xzHS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    \"\"\"\n",
        "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
        "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
        "    \n",
        "    Arguments:\n",
        "    X -- array of sentences (strings), of shape (m, 1)\n",
        "    word_to_index -- a dictionary containing the each word mapped to its index\n",
        "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
        "    \n",
        "    Returns:\n",
        "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Make translation table to replace puctuation\n",
        "    replace_char = {key: None for key in string.punctuation}\n",
        "    replace_char['\"'] = None\n",
        "    table = str.maketrans(replace_char)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    \n",
        "    \n",
        "    m = len(X)                                   # number of training examples\n",
        "    \n",
        "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
        "    X_indices = np.zeros(shape=(m, max_len))\n",
        "    \n",
        "    for i in range(m):                               # loop over training examples\n",
        "        \n",
        "        # Remove punctuation\n",
        "        X[i] = X[i].translate(table)\n",
        "        \n",
        "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
        "        sentence_words = X[i].lower().split()        \n",
        "        \n",
        "        # Initialize j to 0\n",
        "        j = 0\n",
        "        \n",
        "        # Store indices of unknown words in list and then replace it by\n",
        "        # the average of all words\n",
        "        unknown_words_index = []\n",
        "        \n",
        "        # Loop over the words of sentence_words\n",
        "        for w in sentence_words:\n",
        "            # Skip Stopwords\n",
        "            if w in stop_words:\n",
        "              continue\n",
        "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "            if w in word_to_index and j < max_len:\n",
        "              X_indices[i, j] = word_to_index[w]\n",
        "            else:\n",
        "              # Handle unknown key (keep it as zeros)\n",
        "              pass\n",
        "            # Increment j to j + 1\n",
        "            j += 1            \n",
        "    \n",
        "    return X_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5yp6nZaoxzDq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_len = len(max(reviews_train, key=len).split())\n",
        "print(max_len)\n",
        "max_len = 80"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U4q9Y9wVxzAA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert word to indices\n",
        "reviews_train = sentences_to_indices(reviews_train, word_to_index, max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NOE4kGhjxy4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
        "    \n",
        "    Arguments:\n",
        "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    embedding_layer -- pretrained layer Keras instance\n",
        "    \"\"\"\n",
        "    \n",
        "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
        "    \n",
        "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
        "    emb_matrix = np.zeros(shape=(vocab_len, emb_dim))\n",
        "    \n",
        "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "    for word, index in word_to_index.items():\n",
        "        emb_matrix[index, :] = word_to_vec_map[word]\n",
        "\n",
        "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
        "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
        "    \n",
        "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
        "    embedding_layer.build((None,))\n",
        "    \n",
        "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
        "    embedding_layer.set_weights([emb_matrix])\n",
        "    \n",
        "    return embedding_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AyCrlCMnxy0R",
        "colab_type": "code",
        "outputId": "f9f84af2-e837-405f-bc5d-edf766339f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "\n",
        "input_sent = Input(shape=(max_len,))\n",
        "\n",
        "# Create the embedding layer pretrained with glove Vectors (≈1 line)\n",
        "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "    \n",
        "# Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
        "embeddings = embedding_layer(input_sent)\n",
        "\n",
        "X = Bidirectional(GRU(128))(embeddings)\n",
        "X = Dropout(0.2)(X)\n",
        "X = Dense(2)(X)\n",
        "X = Activation('softmax')(X)\n",
        "\n",
        "model = Model(inputs = input_sent, outputs = X)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 80)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 80, 200)           80000200  \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 256)               252672    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 514       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 80,253,386\n",
            "Trainable params: 253,186\n",
            "Non-trainable params: 80,000,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "voHjiLNbxyvF",
        "colab_type": "code",
        "outputId": "c9a25eb6-b15b-450c-b273-6f75d953869d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(reviews_train, y_train, epochs = 2, batch_size = 64)\n",
        "model.save('/content/drive/My Drive/Machine Learning Projects/Amazon Sentiment Analysis/checkpoint_2_epoch.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "500000/500000 [==============================] - 2179s 4ms/step - loss: 0.2777 - acc: 0.8828\n",
            "Epoch 2/2\n",
            "500000/500000 [==============================] - 2147s 4ms/step - loss: 0.2260 - acc: 0.9082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZjDAeMSEppAd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/My Drive/Machine Learning Projects/Amazon Sentiment Analysis/checkpoint_100_epoch.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m1Vb81quYmL8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "myZnXNVPYmHN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IvGALpsmYmFE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "spcrmqZwYmA2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VciYQmFypovB",
        "colab_type": "code",
        "outputId": "b5b73300-4f49-4497-c0e8-3987be1ddc27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/content/drive/My Drive/Machine Learning Projects/Amazon Sentiment Analysis/'\n",
        "# code to train model on multiple small data files\n",
        "train_file = open(BASE_PATH+'data/seperated_data/data_1', 'r')\n",
        "reviews_train, y_train = splitLabelsReviews([x for x in train_file.readlines() if x != '\\n'])\n",
        "# shuffle data\n",
        "reviews_train, y_train = shuffle(reviews_train, y_train)\n",
        "# convert y to numpy\n",
        "y_train = np.array(y_train)\n",
        "# data stats\n",
        "print('Number of train reviews ' + str(len(reviews_train)))\n",
        "max_len = 80\n",
        "# convert word to indices\n",
        "reviews_train = sentences_to_indices(reviews_train, word_to_index, max_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500000/500000 [00:09<00:00, 53536.30it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of train reviews 500000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "po86iZoxYlAk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load previous trained model\n",
        "model = load_model('/content/drive/My Drive/Machine Learning Projects/Amazon Sentiment Analysis/checkpoint_2_epoch_data_2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ftTq7YukZLSb",
        "colab_type": "code",
        "outputId": "6d56992d-b41e-43d2-9113-57f2198eb8cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(reviews_train, y_train, epochs = 2, batch_size = 64)\n",
        "model.save('/content/drive/My Drive/Machine Learning Projects/Amazon Sentiment Analysis/checkpoint_2_epoch_data_1.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "500000/500000 [==============================] - 2646s 5ms/step - loss: 0.2265 - acc: 0.9084\n",
            "Epoch 2/2\n",
            "500000/500000 [==============================] - 2639s 5ms/step - loss: 0.2043 - acc: 0.9182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2kZ7ALaUZWlq",
        "colab_type": "code",
        "outputId": "2794132c-65d3-4613-ca3a-e236936bdc38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "cell_type": "code",
      "source": [
        "# code to train model on multiple small data files\n",
        "for i in range(3, 8):\n",
        "  print('Starting training on data '+str(i))\n",
        "  train_file = open('/content/data/seperated_data/data_'+str(i), 'r')\n",
        "  reviews_train, y_train = splitLabelsReviews([x for x in train_file.readlines() if x != '\\n'])\n",
        "  train_file.close()\n",
        "  # shuffle data\n",
        "  reviews_train, y_train = shuffle(reviews_train, y_train)\n",
        "  # convert y to numpy\n",
        "  y_train = np.array(y_train)\n",
        "  # data stats\n",
        "  print('Number of train reviews ' + str(len(reviews_train)))\n",
        "  max_len = 80\n",
        "  # convert word to indices\n",
        "  reviews_train = sentences_to_indices(reviews_train, word_to_index, max_len)\n",
        "  \n",
        "  model.fit(reviews_train, y_train, epochs = 2, batch_size = 64)\n",
        "  model.save('checkpoint_2_epoch_data_'+str(i)+'.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training on data 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500000/500000 [00:09<00:00, 55275.84it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of train reviews 500000\n",
            "Epoch 1/2\n",
            "500000/500000 [==============================] - 2178s 4ms/step - loss: 0.2125 - acc: 0.9141\n",
            "Epoch 2/2\n",
            "500000/500000 [==============================] - 2189s 4ms/step - loss: 0.1927 - acc: 0.9233\n",
            "Starting training on data 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500000/500000 [00:08<00:00, 58338.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of train reviews 500000\n",
            "Epoch 1/2\n",
            "500000/500000 [==============================] - 2178s 4ms/step - loss: 0.2115 - acc: 0.9151\n",
            "Epoch 2/2\n",
            "500000/500000 [==============================] - 2176s 4ms/step - loss: 0.1924 - acc: 0.9239\n",
            "Starting training on data 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500000/500000 [00:08<00:00, 55993.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of train reviews 500000\n",
            "Epoch 1/2\n",
            "500000/500000 [==============================] - 2239s 4ms/step - loss: 0.2077 - acc: 0.9169\n",
            "Epoch 2/2\n",
            "500000/500000 [==============================] - 2166s 4ms/step - loss: 0.1888 - acc: 0.9251\n",
            "Starting training on data 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500000/500000 [00:08<00:00, 60098.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of train reviews 500000\n",
            "Epoch 1/2\n",
            "500000/500000 [==============================] - 2082s 4ms/step - loss: 0.2076 - acc: 0.9172\n",
            "Epoch 2/2\n",
            "193920/500000 [==========>...................] - ETA: 20:19 - loss: 0.1855 - acc: 0.9274Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kCKHmZ3hZWhX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp /content/checkpoint_2_epoch_data_*.h5 \"/content/drive/My Drive/Machine Learning Projects/Amazon Sentiment Analysis\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HJXMsFH1ZWdY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}